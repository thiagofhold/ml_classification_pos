
##setting work directory
setwd("~/Desktop/projeto_fred_fmu/ml_classification_pos/forest_firing/bases")

######################################################################################

#Installing packages 
install.packages("ggplot2")
install.packages("lattice")
install.packages('caret', dependencies = TRUE)
library(caret)


######################################################################################

##importando dataset e atribuindo numa variavel
amazon_f <- read.csv("fires_trt.csv",sep = ",")

##checking the structure of the dataset
str(amazon_f)
summary(amazon_f)
head(amazon_f)
colnames(amazon_f)


# converting the columns to factors (transforming the dataset)
amazon_f$year <- as.factor(amazon_f$year)
amazon_f$state <- as.factor(amazon_f$state)
amazon_f$trt_region <- as.factor(amazon_f$trt_region)
amazon_f$trt_month_year <- as.factor(amazon_f$trt_month_year)
amazon_f$trt_number <- as.factor(amazon_f$trt_number)
amazon_f$trt_estacoes <- as.factor(amazon_f$trt_estacoes)


str(amazon_f)
summary(amazon_f)


######################################################################################
# Data Partition

set.seed(123)
# especifica que queremos 2 grupos utilisando a funcao sample "c" specifies the way we want the 2 groups to be devided.
ind <- sample(2, nrow(amazon_f), replace = TRUE, prob = c(0.7, 0.3))
train <- amazon_f[ind==1,]
test <- amazon_f[ind==2,]

str(test)


######################################################################################
#modelos de classificacao

#Naive Bayes 
library(e1071)
fit_nb <- naiveBayes(trt_number ~ ., data = train)

# K-Nearest Neighbors
library(caret)
fit_knn30 <- knn3(trt_number ~ ., data = train, k = 30)
fit_knn100 <- knn3(trt_number ~ ., data = train, k = 100)

#lda
library(MASS)
fit_lda <- lda(trt_number ~ ., data = train)

# SVM
library(e1071)
fit_svm_linear <- svm(trt_number ~ ., data = train, kernel="linear", probability = TRUE)

fit_svm_rbf <- svm(trt_number ~ ., data = train, kernel = "radial", probability = TRUE)

fit_svm_poly <- svm(trt_number ~ ., data = train, kernel = "polynomial", probability = TRUE)

fit_svm_sig <- svm(trt_number ~ ., data = train, kernel = "sigmoid", probability = TRUE)


######################################################################################
# Predictions 


# NaiveBayes
pred_nb <- predict(fit_nb , test, type = 'raw')[,2]

# KNN
pred_knn30 <- predict(fit_knn30 , test, type = 'prob')[,2]
pred_knn100 <- predict(fit_knn100 , test, type = 'prob')[,2]

#lda
pred_lda <- predict(fit_lda , test, type = 'response')$posterior[,2]

# SVM -- Support Vector Machines
pred_svm_linear <- attr(predict(fit_svm_linear , test, type = 'prob', probability = TRUE), "probabilities")[,1]
pred_svm_rbf <- attr(predict(fit_svm_rbf , test, type = 'prob', probability = TRUE), "probabilities")[,1]
pred_svm_poly <- attr(predict(fit_svm_poly , test, type = 'response', probability = TRUE), "probabilities")[,1]
pred_svm_sig <- attr(predict(fit_svm_sig , test, type = 'response', probability = TRUE), "probabilities")[,1]

######################################################################################
#plotando ROC.

library(ROCit)
# NaiveBayes
plot(rocit(score=pred_nb,class=test$trt_number))

# KNN
plot(rocit(score=pred_knn30,class=test$trt_number))
plot(rocit(score=pred_knn100,class=test$trt_number))

#lda
plot(rocit(score=pred_lda,class=test$trt_number))

# SVM -- Support Vector Machines
plot(rocit(score=pred_svm_linear,class=test$trt_number))
plot(rocit(score=pred_svm_rbf,class=test$trt_number))
plot(rocit(score=pred_svm_poly,class=test$trt_number))
plot(rocit(score=pred_svm_sig,class=test$trt_number))
